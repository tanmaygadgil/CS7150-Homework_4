{"cells":[{"cell_type":"markdown","metadata":{},"source":["This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework_4/blob/main/Assignment_4_Transformers.ipynb)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MjJqMmmUQq3S"},"source":["# Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHghnPFNQtqz"},"outputs":[],"source":["!pip install -U spacy==3.6.0\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm\n","!pip install torchdata\n","!pip install -U torchtext\n","!pip install portalocker>=2.0.0\n","!pip install seaborn"]},{"cell_type":"markdown","metadata":{"id":"tUro-YgqurFf"},"source":["# Transformer Assignment\n","\n","## Overview\n","\n","In this assignment, you will be trying your hand at understanding transformers, their architecture, and their difference in-terms of basic RNNs. The assignment is divided in 2 sections.\n","\n","\n","*   Section 1:\n","\n"," You will be implmenting a basic RNN cell, RNN Class and an RNN Classifier\n","\n","\n","*   Section 2:\n","\n","  You will be implementing a Transformer based Text classifier using components such as Multi-head Attention Module, Positional Encoding Module and Encoder\n","\n","*   Section 3:\n","\n","  In order to experiment with Decoders for a Transformer, we will be implementing a Transformer Based Machine Translation class using modules of Section 2, a Decoder, Attention Masks and Seq-Seq Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ln34q0UvNCJJ"},"outputs":[],"source":["import math\n","import torch\n","import time\n","\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import seaborn as sns\n","\n","from torch.utils.data import DataLoader\n","from torchtext.datasets import AG_NEWS\n","from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import Multi30k\n","from typing import Iterable, List"]},{"cell_type":"markdown","metadata":{"id":"jkqkCJ3s1pt5"},"source":["# Section 1: Recurrent Neural Networks (RNN)\n"]},{"cell_type":"markdown","metadata":{"id":"j5YvJ8p54BVw"},"source":["\n","Each RNN Cell should contain 2 components: an Input Unit and a Hidden Unit. The Hidden state is the part of the RNN that remembers context about previous data present in the sequence. The current time step's hidden state is calculated using information of the previous time steps hidden state and the current input. This process helps to retain information on what the model saw in the previous time step when processing the current time steps information.\n","\n","RNNs will look and function as follows,\n","\n","<img src=\"images\\RNN Multi.png\">\n","\n","The hidden state any given time *t* is given by,\n","\n","\\begin{align}\n","input_t &= (x_t \\cdot W_x^t + b_x^t) \\\\\n","prev\\_state &= (h_{t-1} \\cdot W^t_h + b_h^t) \\\\\n","\\end{align}\n","\n","\n","\\begin{align}\n","h_{t} &= tanh( input_t+ prev\\_state)\n","\\end{align}\n","\n","The output at any give time *t* is given by,\n","\n","\\begin{align}\n","y_t = h_t \\cdot W_y^t + b_y\n","\\end{align}\n","\n","Note:  All the connections in RNN have weights and biases.\n","\n","Your job is to implement the formulae above."]},{"cell_type":"markdown","metadata":{"id":"g67cCQBU3c_p"},"source":["## 1.1 A Single RNN Cell"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcyGQaLpulEt"},"outputs":[],"source":["class RNNCell(torch.nn.Module):\n","    \"\"\"\n","    RNNCell is a single cell that takes x_t and h_{t_1} as input and outputs h_t.\n","    \"\"\"\n","    def __init__(self, input_dim: int, hidden_dim: int):\n","        \"\"\"\n","        Constructor of RNNCell.\n","\n","        Inputs:\n","        - input_dim: Dimension of the input x_t\n","        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n","        \"\"\"\n","\n","        # We always need to do this step to properly implement the constructor\n","        super(RNNCell, self).__init__()\n","\n","        self.linear_x, self.linear_h, self.non_linear = None, None, None\n","\n","        ###########################################################################\n","        # TODO:\n","        # 1. Define the linear transformation layers for the attributes\n","        #    (set to None above) to correspond to the W_x and W_h in the formulae.\n","        #    Remember to include bias in the linear layers.\n","        #    (Refer to nn.Linear documentation https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","        # 2. Define the non_linear layer. (You can use tanh as describe bove).\n","        ###########################################################################\n","\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, x_cur: torch.Tensor, h_prev: torch.Tensor):\n","        \"\"\"\n","        Compute h_t given x_t and h_{t-1}.\n","\n","        Inputs:\n","        - x_cur: x_t, a tensor with the same of BxC, where B is the batch size and\n","          C is the channel dimension.\n","        - h_prev: h_{t-1}, a tensor with the same of BxH, where H is the channel\n","          dimension.\n","        \"\"\"\n","        h_cur = None\n","        ###########################################################################\n","        # TODO: Run the linear transformation layers to compute x_t and consume\n","        # h_{t-1}               #\n","        # go  non-linear layer.                                                   #\n","        ###########################################################################\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","        return h_cur"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnUpMZYP4tjd"},"outputs":[],"source":["# Let's run a sanity check of your model\n","x = torch.randn((2, 8)) # Input Dim\n","h = torch.randn((2, 16)) # Hidden Dim\n","\n","model = RNNCell(8, 16)\n","y = model(x , h)\n","assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == 16\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"wtKx0smPNlBO"},"source":["## 1.2 RNN Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rx2h1DoaNlRY"},"outputs":[],"source":["class RNN(torch.nn.Module):\n","    \"\"\"\n","    RNN is a single-layer (stack) RNN by connecting multiple RNNCell together in a single\n","    direction, where the input sequence is processed from left to right.\n","    \"\"\"\n","    def __init__(self, input_dim: int, hidden_dim: int):\n","        \"\"\"\n","        Constructor of the RNN module.\n","\n","        Inputs:\n","        - input_dim: Dimension of the input x_t\n","        - hidden_dim: Dimension of the hidden state h_{t-1} and h_t\n","        \"\"\"\n","        super(RNN, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        ###########################################################################\n","        # TODO: Define the RNNCell.                                               #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, x: torch.Tensor):\n","        \"\"\"\n","        Compute the hidden representations for every token in the input sequence.\n","\n","        Input:\n","        - x: A tensor with the shape of BxLxC, where B is the batch size, L is the squence\n","          length, and C is the channel dimmension\n","\n","        Return:\n","        - h: A tensor with the shape of BxLxH, where H is the hidden dimension of RNNCell\n","        \"\"\"\n","        b = x.shape[0]\n","        seq_len = x.shape[1]\n","\n","        # initialize the hidden dimension\n","        init_h = x.new_zeros((b, self.hidden_dim))\n","        h = None\n","        ###########################################################################\n","        # TODO: Compute the hidden representation for every token in the input    #\n","        # from left to right as per the formula stated above                      #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699287078348,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"pTrPvUkEOXrR","outputId":"f6d3c65b-be2f-49df-82a2-00f0911fee5b"},"outputs":[],"source":["# Let's run a sanity check of your model\n","x = torch.randn((2, 10, 8))\n","model = RNN(8, 16)\n","y = model(x)\n","assert len(y.shape) == 3\n","for dim, dim_gt in zip(y.shape, [2, 10, 16]):\n","    assert dim == dim_gt\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"gySwzL67ObkF"},"source":["## 1.3 RNN Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gyqFZKgOaoR"},"outputs":[],"source":["h_tracker = {}\n","\n","class RNNClassifier(nn.Module):\n","    \"\"\"\n","    A RNN-based classifier for text classification. It first converts tokens into word embeddings.\n","    And then feeds the embeddings into a RNN, where the hidden representations of all tokens are\n","    then averaged to get a single embedding of the sentence. It will be used as input to a linear\n","    classifier.\n","    \"\"\"\n","    def __init__(self,\n","            vocab_size: int, embed_dim: int, rnn_hidden_dim: int, num_class: int, pad_token: int\n","        ):\n","        \"\"\"\n","        Constructor.\n","\n","        Inputs:\n","        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n","        - embed_dim: The dimension of word embeddings\n","        - rnn_hidden_dim: The hidden dimension of the RNN.\n","        - num_class: Number of classes.\n","        - pad_token: The index of the padding token.\n","        \"\"\"\n","        super(RNNClassifier, self).__init__()\n","\n","        # word embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n","\n","        self.rnn, self.fc = None, None\n","\n","        ###########################################################################\n","        # TODO: Define the RNN and the classification layer.                      #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def init_weights(self):\n","        initrange = 0.5\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\n","        self.fc.weight.data.uniform_(-initrange, initrange)\n","        self.fc.bias.data.zero_()\n","\n","    def forward(self, text):\n","        \"\"\"\n","        Get classification scores (logits) of the input.\n","\n","        Input:\n","        - text: Tensor with the shape of BxLxC.\n","\n","        Return:\n","        - logits: Tensor with the shape of BxK, where K is the number of classes\n","        \"\"\"\n","\n","        # get word embeddings\n","        embedded = self.embedding(text)\n","\n","        logits = None\n","        ###########################################################################\n","        # TODO: Compute logits of the input.                                      #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1699287078605,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"j3Bu8qExOwmO","outputId":"b895fa43-33c5-4e4a-eff4-89b01e6aa80e"},"outputs":[],"source":["# Sanity check!!!\n","vocab_size = 10\n","embed_dim = 16\n","rnn_hidden_dim = 32\n","num_class = 4\n","\n","x = torch.arange(vocab_size).view(1, -1)\n","x = torch.cat((x, x), dim=0)\n","print('x.shape: {}'.format(x.shape))\n","model = RNNClassifier(vocab_size, embed_dim , rnn_hidden_dim, num_class, 0)\n","y = model(x)\n","assert len(y.shape) == 2 and y.shape[0] == 2 and y.shape[1] == num_class\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"6zZ-wowAQcmI"},"source":["## Data Loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15361,"status":"ok","timestamp":1699287093964,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"A7Uhb_tcQewa","outputId":"f6e6035e-a8ae-441f-cdc7-4469587073cb"},"outputs":[],"source":["# check here for details https://github.com/pytorch/text/blob/main/torchtext/data/utils.py#L52-#L166\n","from torchtext.data.utils import get_tokenizer\n","# check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py#L65-L113\n","from torchtext.vocab import build_vocab_from_iterator\n","# Documentation of DataLoader https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n","from torch.utils.data import DataLoader\n","\n","# A tokenizer splits a input setence into a set of tokens, including those puncuation\n","# For example\n","# >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n","# >>> tokens\n","# >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n","tokenizer = get_tokenizer('basic_english')\n","\n","train_iter = AG_NEWS(split='train')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","# Creates a vocab object which maps tokens to indices\n","# Check here for details https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab.py\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n","\n","# The specified token will be returned when a out-of-vocabulary token is queried.\n","vocab.set_default_index(vocab[\"<unk>\"])\n","\n","text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1\n","\n","# The padding token we need to use\n","# The returned indices are always in an array\n","PAD_TOKEN = vocab(tokenizer('<pad>'))\n","assert len(PAD_TOKEN) == 1\n","PAD_TOKEN = PAD_TOKEN[0]\n","\n","\n","# Merges a list of samples to form a mini-batch of Tensor(s)\n","def collate_batch(batch):\n","    \"\"\"\n","    Input:\n","    - batch: A list of data in a mini batch, where the length denotes the batch size.\n","      The actual context depends on a particular dataset. In our case, each position\n","      contains a label and a Tensor (tokens in a sentence).\n","\n","    Returns:\n","    - batched_label: A Tensor with the shape of (B,)\n","    - batched_text: A Tensor with the shape of (B, L, C), where L is the sequence length\n","      and C is the channeld dimension\n","    \"\"\"\n","    label_list, text_list, text_len_list = [], [], []\n","    for (_label, _text) in batch:\n","        label_list.append(label_pipeline(_label))\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text)\n","        text_len_list.append(processed_text.size(0))\n","    batched_label, batched_text = None, None\n","    ###########################################################################\n","    # TODO: Pad the text tensor in the mini batch so that they have the same  #\n","    # length. Specifically, you need to calculate the maximum length in the   #\n","    # batch and then add the token PAD_TOKEN to the end of those              #\n","    # shorter sentences. (Try printing a few data points to understand why)   #\n","    ###########################################################################\n","    raise NotImplementedError\n","\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","\n","    return batched_label.long(), batched_text.long()\n","\n","# Now, let's check what the batched data looks like\n","train_iter = AG_NEWS(split='train')\n","dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n","for idx, (label, data) in enumerate(dataloader):\n","    if idx > 0:\n","        break\n","    print('label.shape: {}'.format(label.shape))\n","    print('label: {}'.format(label))\n","    print('data.shape: {}'.format(data.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4782,"status":"ok","timestamp":1699287098736,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"O8gaIhlS-KPz","outputId":"d092d9a5-ccff-4089-aef1-dfba9ce69f18"},"outputs":[],"source":["labels=set()\n","labels.update([entry[0] for entry in AG_NEWS(root=\"data\")[0]])\n","print(labels)"]},{"cell_type":"markdown","metadata":{"id":"k90mvB7GPk2x"},"source":["## 1.4 Train & Evaluate Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBeUMCGnPkEx"},"outputs":[],"source":["\n","# logits_tracker = {}\n","def train(model, dataloader, loss_func, device, grad_norm_clip, optimizer):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","    start_time = time.time()\n","    global logits_tracker\n","\n","    for idx, (label, text) in enumerate(dataloader):\n","        label = label.to(device)\n","        text = text.to(device)\n","        optimizer.zero_grad()\n","\n","        logits = None\n","        ###########################################################################\n","        # TODO: compute the logits of the input, get the loss, and do the         #\n","        # gradient backpropagation.\n","        ###########################################################################\n","        raise NotImplementedError\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n","        optimizer.step()\n","        total_acc += (logits.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n","                                              total_acc/total_count))\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(model, dataloader, loss_func, device):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text) in enumerate(dataloader):\n","            label = label.to(device)\n","            text = text.to(device)\n","\n","            ###########################################################################\n","            # TODO: compute the logits of the input, get the loss.                    #\n","            ###########################################################################\n","\n","            raise NotImplementedError\n","            ###########################################################################\n","            #                             END OF YOUR CODE                            #\n","            ###########################################################################\n","\n","            total_acc += (logits.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc/total_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrskpzBbRnu6"},"outputs":[],"source":["\n","assert torch.cuda.is_available(), \"Please connect to the GPU instance if working on Colab or configure the environment for Torch using GPU (Comment this line if not using GPU)\"\n","# device = 'cuda'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyper parameters\n","epochs = 3 # epoch\n","lr =0.0005 # learning rate\n","batch_size = 64 # batch size for training\n","word_embed_dim = 64\n","rnn_hidden_dim = 96\n","\n","train_iter = AG_NEWS(split='train')\n","num_class = len(set([label for (label, text) in train_iter]))\n","vocab_size = len(vocab)\n","\n","model, loss_func = None, None\n","\n","###########################################################################\n","# TODO: Define the classifier and loss function.\n","###########################################################################\n","\n","raise NotImplementedError\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################\n","\n","# copy the model to the specified device (GPU)\n","model = model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n","total_accu = None\n","train_iter, test_iter = AG_NEWS()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","num_train = int(len(train_dataset) * 0.95)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1699287109588,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"Jw0UjFJ6SXIH","outputId":"01088211-3a0e-4d39-93f7-a1ca33e62718"},"outputs":[],"source":["split_train_, split_valid_ = random_split(\n","    train_dataset,\n","    [num_train, len(train_dataset) - num_train]\n",")\n","\n","train_dataloader = DataLoader(\n","    split_train_, batch_size=batch_size,\n","    shuffle=True, collate_fn=collate_batch\n",")\n","\n","valid_dataloader = DataLoader(\n","    split_valid_, batch_size=batch_size,\n","    shuffle=False, collate_fn=collate_batch\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset, batch_size=batch_size,\n","    shuffle=False, collate_fn=collate_batch\n",")\n","split_train_[21]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294881,"status":"ok","timestamp":1699287404459,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"_bR_1gj7SXvN","outputId":"4d1eb428-c9bc-46cf-8fee-5a41dd9dad97"},"outputs":[],"source":["# You should be able get a validation accuracy around 86%\n","for epoch in range(1, epochs + 1):\n","    # global logits_tracker\n","    # logits_tracker[epoch] = None\n","    epoch_start_time = time.time()\n","    train(model, train_dataloader, loss_func, device, 1, optimizer)\n","    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n","    if total_accu is not None and total_accu > accu_val:\n","        scheduler.step()\n","    else:\n","        total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"]},{"cell_type":"markdown","metadata":{"id":"g2Ca9byg078p"},"source":["# Section 2: Transformers - 'Attention is All you Need' : Classifier"]},{"cell_type":"markdown","metadata":{},"source":["Transformers are a type of deep learning architecture that has had a profound impact on a wide range of natural language processing (NLP) tasks and other sequence-to-sequence tasks. They are known for their ability to model long-range dependencies and their parallelization capabilities. A typical transformer model consists of several key components:\n","\n","<img src=\"images\\The-Transformer-model-architecture.png\">\n","\n","Transformers have revolutionized NLP and have been adapted for a wide range of applications beyond text, including image generation, recommendation systems, and more. For this section, we will be implementing, all but two important components, Attention-Masks and Decoder Module. They will be implemented in-depth in Section 3."]},{"cell_type":"markdown","metadata":{"id":"Ldgv5vkemPN6"},"source":["## 2.1 Multihead Attention"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"images\\MHA.png\" width=300>"]},{"cell_type":"markdown","metadata":{"id":"tmKX9MmKpTAl"},"source":["Multi-Head Attention can be mathematically explained as follows:\n","\n","Let's assume we have a sequence of input vectors $(X = {x_1, x_2, ...., x_n})$, where ($x_i$) represents the ($i$)-th element of the sequence. Each $x_i$ is typically a vector, such as a word embedding in natural language processing.\n","\n","\n","\n","1. **Single Attention Head:**\n","   - In a single attention head, we compute attention scores ($A_{ij}$) between every pair of input elements ($x_i$) and ($x_j$). These scores are computed using a compatibility function, often a dot product or a learned linear transformation followed by a softmax activation:\n","\n","   $\\begin{align}\n","   A_{ij} = {softmax}({(Q_ix_i)^T(K_jx_j)}{\\sqrt{d_k}})\n","  \\end{align}\n","   $\n","   Where $Q_i$ and $K_j$ are learned linear transformations of the input vectors $x_i$ and $x_j$, and $d_k$ is the dimension of the key vectors.\n","\n","   - The attention scores are used to compute weighted representations of the input sequence:\n","\n","   \\begin{align}\n","   \\text{Attention}(X) = \\sum_{j=1}^{n} A_{ij}V_j\n","   \\end{align}\n","\n","   Where $V_j$ is a learned linear transformation of the input vector $x_j$.\n","\n","2. **Multiple Attention Heads:**\n","   - In Multi-Head Attention, we use $H$ attention heads in parallel. Each head has its own sets of learned parameters for $Q$, $K$, and $V$, resulting in $H$ sets of attention scores and weighted representations.\n","   \n","    \\begin{align}\n","   MultiHead(X) = Concatenate(Head_1, Head_2,..., Head_H).W^O\n","   \\end{align}\n","\n","   Where $W^O$ is another learned linear transformation applied to the concatenated outputs, and $Head_i$ represents the output of the $i$-th attention head.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBnKpnQ5lplW"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    A module that computes multi-head attention given query, key, and value tensors.\n","    \"\"\"\n","    def __init__(self, input_dim: int, num_heads: int):\n","        \"\"\"\n","        Constructor.\n","\n","        Inputs:\n","        - input_dim: Dimension of the input query, key, and value. Here we assume they all have\n","          the same dimensions. But they could have different dimensions in other problems.\n","        - num_heads: Number of attention heads\n","        \"\"\"\n","        super(MultiHeadAttention, self).__init__()\n","\n","        assert input_dim % num_heads == 0 # Check if we can get back the original Dimensions!\n","\n","        self.input_dim = input_dim\n","        self.num_heads = num_heads\n","        self.dim_per_head = input_dim // num_heads\n","\n","        ###########################################################################\n","        # TODO: Define the linear transformation layers for key, value, and query.#\n","        # Also define the output layer.\n","        ###########################################################################\n","\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        self.scores = None\n","\n","    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None):\n","        \"\"\"\n","        Compute the attended feature representations.\n","\n","        Inputs:\n","        - query: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n","          and C is the channel dimension\n","        - key: Tensor of the shape BxLxC\n","        - value: Tensor of the shape BxLxC\n","        - mask: Tensor indicating where the attention should *not* be performed\n","        \"\"\"\n","        b = query.shape[0]\n","\n","        dot_prod_scores = None\n","        ###########################################################################\n","        # TODO: Compute the scores based on dot product between transformed query,#\n","        # key, and value. You may find torch.matmul helpful, whose documentation  #\n","        # can be found at                                                         #\n","        # https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul#\n","        # Remember to devide the doct product similarity scores by square root of #\n","        # the channel dimension per head.\n","        #                                                                         #\n","        # Since no for loops are allowed here, think of how to use tensor reshape #\n","        # to process multiple attention heads at the same time.                   #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        if mask is not None:\n","            # We simply set the similarity scores to be near zero for the positions\n","            # where the attention should not be done. Think of why we do this.\n","            dot_prod_scores = dot_prod_scores.masked_fill(mask == 0, -1e9)\n","\n","        out = None\n","        ###########################################################################\n","        # TODO: Compute the attention scores, which are then used to modulate the #\n","        # value tensor. Finally concate the attended tensors from multiple heads  #\n","        # and feed it into the output layer. You may still find torch.matmul      #\n","        # helpful.                                                                #\n","        #                                                                         #\n","        # Again, think of how to use reshaping tensor to do the concatenation.    #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNS1Elc9wvhc"},"outputs":[],"source":["# Sanity Check\n","x = torch.randn((2, 10, 8))\n","mask = torch.randn((2, 10)) > 0.5\n","mask = mask.unsqueeze(1).unsqueeze(-1)\n","num_heads = 4\n","model = MultiHeadAttention(8, num_heads)\n","y = model(x, x, x, mask)\n","assert len(y.shape) == len(x.shape)\n","for dim_x, dim_y in zip(x.shape, y.shape):\n","    assert dim_x == dim_y"]},{"cell_type":"markdown","metadata":{"id":"LuRsu7Usw8hC"},"source":["## 2.2 Positional Encoding Module"]},{"cell_type":"markdown","metadata":{"id":"sq0T_lmV-hT3"},"source":["Positional Encoding is a critical component in the Transformer architecture, designed to provide information about the positions of elements in a sequence to a model that inherently lacks sequential information. Transformers use self-attention mechanisms that do not inherently understand the order or position of tokens in the input. Positional Encoding is introduced to address this limitation and allow the model to consider the order of elements within the input sequence.\n","\n","It addresses the challenge of modeling sequences with self-attention mechanisms that do not inherently understand the order of elements. By adding Positional Encoding to the input embeddings, the model can differentiate between tokens based on their positions and capture sequential information effectively.\n","\n","\n","1. **Positional Encoding Function:**\n","   - Positional Encoding is typically represented as a fixed-size vector that is added element-wise to the input embeddings. This vector is determined by a mathematical function.\n","   - The most common approach is to use a combination of sine and cosine functions with different frequencies and phases to create a unique encoding for each position.\n","   - For each position $pos$ and dimension $i$ of the Positional Encoding vector, $PE(pos, 2i)$ is given by\n","   \n","  \\begin{align}\n","  \\sin\\left(\\frac{pos}{10000^{(2i/d_{\\text{model}})}}\\right) & \\text{if } i \\text{ is even} \\\\\n","  \\cos\\left(\\frac{pos}{10000^{(2i/d_{\\text{model}})}}\\right) & \\text{if } i \\text{ is odd}\n","  \\end{align}\n","\n"," *$d_{model}$ is the dimension of the model's input embeddings.*\n","\n","3. **Adding Positional Encoding:**\n","   - The Positional Encoding vector is added element-wise to the input embeddings. This combination of the original word embeddings and the Positional Encoding allows the model to distinguish between tokens based on their positions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w3tNlP38HbML"},"source":["### 2.2.1 Let's Try to work an example!\n","Assume the sentence coming into the encoding is, \"This is an Example\"; The Positional Encoding layer is initialized with the following parameters;\n","\n","* $k$ = $0\\leq k < L $ =$\\boxed{TODO:\\text{Write the max value k can take}}$\n","* $n = 100$\n","* $d = 6$\n","* $I$ = $0 \\leq i < d/2$ = $\\boxed{TODO: \\text{Write the max value i can take}}$\n","*  Based on the values above, for the text given, Find the Position Encoding values below or create on on your own and upload to this section! ***DONT CODE IT***\n","\n","|          |   |           |   | Positional Encodings d = 6 & n = 100 |     |     |     |     |     |     |     |\n","|----------|---|-----------|---|:------------------------------------:|-----|-----|-----|-----|-----|-----|-----|\n","| Sequence |   | Index (k) |   | i=0                                  | i=0 | i=1 | i=1 | i=2 | i=2 | i=3 | i=3 |\n","| This     |   | 0         |   | $P_{00} = 0$   |\n","| is       |   | 1         |   | $P_{10}$ =0.841 |\n","| an       |   | 2         |   | $P_{20} = 0.909$|              \n","| Example  |   | 3         |   | $P_{30} = 0.141$|"]},{"cell_type":"markdown","metadata":{"id":"-Td0eavTHgs1"},"source":["### 2.2.1 Let's Code!\n","\n","Now try to use the same approach to implement the Positional Encoding part.\n","\n","For full credit do not use for loops;\n","\n","Make use of packages like\n","* torch.arange() : https://pytorch.org/docs/stable/generated/torch.arange.html\n","* torch.stack() : https://pytorch.org/docs/stable/generated/torch.stack.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zhA3ED4w_0H"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    A module that adds positional encoding to each of the token's features.\n","    So that the Transformer is position aware.\n","    \"\"\"\n","    def __init__(self, input_dim: int, max_len: int=10000):\n","        \"\"\"\n","        Inputs:\n","        - input_dim: Input dimension about the features for each token\n","        - max_len: The maximum sequence length\n","        \"\"\"\n","        super(PositionalEncoding, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.max_len = max_len\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Compute the positional encoding and add it to x.\n","\n","        Input:\n","        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n","          and C is the channel dimension\n","\n","        Return:\n","        - x: Tensor of the shape BxLxC, with the positional encoding added to the input\n","        \"\"\"\n","        seq_len = x.shape[1]\n","        input_dim = x.shape[2]\n","\n","        pe = None\n","        ###########################################################################\n","        # TODO: Compute the positional encoding                                   #\n","        # Check Section 3.5 for the definition (https://arxiv.org/pdf/1706.03762.pdf)\n","        #                                                                         #\n","        # It's a bit messy, but the definition is provided for your here for your #\n","        # convenience (in LaTex).                                                 #\n","        # PE_{(pos,2i)} = sin(pos / 10000^{2i/\\dmodel})                           #\n","        # PE_{(pos,2i+1)} = cos(pos / 10000^{2i/\\dmodel})                         #\n","        #                                                                         #\n","        # You should replace 10000 with max_len here.\n","        ###########################################################################\n","        raise NotImplementedError\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        x = x + pe.to(x.device)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1AbsDpAxEmi"},"outputs":[],"source":["# Sanity check - I\n","x = torch.randn(1, 100, 20)\n","pe = PositionalEncoding(20)\n","y = pe(x)\n","assert len(x.shape) == len(y.shape)\n","for dim_x, dim_y in zip(x.shape, y.shape):\n","    assert dim_x == dim_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glfitwCKUc2E"},"outputs":[],"source":["# Sanity Check - II\n","x = torch.randn(1, 100, 6)\n","d = 6\n","n = 100\n","pe = PositionalEncoding(d,n)\n","y = pe(x)\n","\n","y -= x\n","print(y[:,:4,:])"]},{"cell_type":"markdown","metadata":{"id":"L1CfurktZuKi"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"executionInfo":{"elapsed":578,"status":"ok","timestamp":1699287405200,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"15RK-geDxGOI","outputId":"9b832038-1a85-40e6-f27d-045b1163115d"},"outputs":[],"source":["# Sanity check - III\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.figure(figsize=(15, 5))\n","pe = PositionalEncoding(20)\n","y = pe.forward((torch.zeros(1, 100, 20)))\n","plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n","plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"]},{"cell_type":"markdown","metadata":{"id":"BSJX3Bnk3Wif"},"source":["<img src=\"images\\PE Wave.png\">"]},{"cell_type":"markdown","metadata":{"id":"GWXhGiwFAVI_"},"source":["## 2.3 FeedForward Module"]},{"cell_type":"markdown","metadata":{"id":"FNloj6m5CpxJ"},"source":["The FeedForward Layer in the Transformer architecture is a position-wise neural network layer designed to process the context-aware representations generated by the self-attention mechanism. It consists of two linear transformations followed by a non-linear activation function, typically ReLU.  The FeedForward Layer is applied independently to each position in the sequence, allowing the model to capture different patterns at different positions. This position-wise independence, combined with non-linearity, helps the model learn complex relationships within the data and plays a crucial role in the Transformer's ability to process and understand sequential data effectively, making it a fundamental component for various sequence-to-sequence tasks.\n","\n","Mathematically, if $X$ represents the input sequence (a sequence of embeddings), $FFN(X)$ is the output of the FeedForward Layer, and $W_1$, $W_2$, $b_1$, and $b_2$ represent learned weight matrices and bias terms, the operation can be expressed as,\n","\n","\\begin{align}\n","FFN(X) = ReLU(X.W_i + b_i) . W_2 + b_2.\n","\\end{align}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnhMfzr_AXDO"},"outputs":[],"source":["class FeedForwardNetwork(nn.Module):\n","    \"\"\"\n","    A simple feedforward network. Essentially, it is a two-layer fully-connected\n","    neural network.\n","    \"\"\"\n","    def __init__(self, input_dim, ff_dim, dropout):\n","        \"\"\"\n","        Inputs:\n","        - input_dim: Input dimension\n","        - ff_dim: Hidden dimension\n","        \"\"\"\n","        super(FeedForwardNetwork, self).__init__()\n","\n","        ###########################################################################\n","        # TODO: Define the two linear layers and a non-linear one.\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, x: torch.Tensor):\n","        \"\"\"\n","        Input:\n","        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n","         and C is the channel dimension\n","\n","        Return:\n","        - y: Tensor of the shape BxLxC\n","        \"\"\"\n","\n","        y = None\n","        ###########################################################################\n","        # TODO: Process the input.                                                #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu1ZP598AZOw"},"outputs":[],"source":["# Sanity Check\n","x = torch.randn((2, 10, 8))\n","ff_dim = 4\n","model = FeedForwardNetwork(8, ff_dim, 0.1)\n","y = model(x)\n","assert len(x.shape) == len(y.shape)\n","for dim_x, dim_y in zip(x.shape, y.shape):\n","    assert dim_x == dim_y\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"LuQTU5FYAeAw"},"source":["## 2.4 Encoder Module"]},{"cell_type":"markdown","metadata":{"id":"yim4YF3JWWPA"},"source":["The Encoder module in a Transformer is responsible for processing the input sequence, typically used for tasks like language understanding and representation learning. It consists of multiple identical layers, each containing two main components: the Multi-Head Self-Attention mechanism and the Position-wise FeedForward Layer.\n","\n","<img src=\"images\\Encoder-module.png\" width=\"300\">\n","\n","*   In each layer, the input sequence is first passed through the Multi-Head Self-Attention mechanism, which computes weighted representations for each element in the sequence, capturing contextual information.The attention output is then passed through the Position-wise FeedForward Layer, introducing non-linearity and allowing the model to capture different patterns at each position.\n","*   This process is repeated for each layer in the encoder stack, enabling the model to capture hierarchical features and   within the input sequence effectively. The final encoder output represents a rich contextualized representation of the input sequence, which can be used for various downstream tasks, including translation, text generation, and sentiment analysis.\n"]},{"cell_type":"markdown","metadata":{"id":"sdP_tuKQHz09"},"source":["### 2.4.1 Encoder Cell"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7h7bo0xlAgEo"},"outputs":[],"source":["class TransformerEncoderCell(nn.Module):\n","    \"\"\"\n","    A single cell (unit) for the Transformer encoder.\n","    \"\"\"\n","    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float):\n","        \"\"\"\n","        Inputs:\n","        - input_dim: Input dimension for each token in a sequence\n","        - num_heads: Number of attention heads in a multi-head attention module\n","        - ff_dim: The hidden dimension for a feedforward network\n","        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n","          modules.\n","        \"\"\"\n","        super(TransformerEncoderCell, self).__init__()\n","\n","        ###########################################################################\n","        # TODO: A single Transformer encoder cell consists of\n","        # 1. A multi-head attention module\n","        # 2. Followed by dropout\n","        # 3. Followed by layer norm (check nn.LayerNorm)\n","        # https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm\n","        #                                                                         #\n","        # At the same time, it also has\n","        # 1. A feedforward network\n","        # 2. Followed by dropout\n","        # 3. Followed by layer norm\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","        self.attention = None\n","\n","    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n","        \"\"\"\n","        Inputs:\n","        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n","          and C is the channel dimension\n","        - mask: Tensor for multi-head attention\n","        \"\"\"\n","\n","        y = None\n","        ###########################################################################\n","        # TODO: Get the output of the multi-head attention part (with dropout     #\n","        # and layer norm), which is used as input to the feedforward network (    #\n","        # again, followed by dropout and layer norm).                             #\n","        #                                                                         #\n","        # Don't forget the residual connections for both parts. Append the        #\n","        # 1st Normalized Output before feed_forward to self.attention(Useful in   #\n","        # visualizing)                                                            #\n","        ###########################################################################\n","\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MofgAqe_Ajzp"},"outputs":[],"source":["# Sanity Check\n","\n","x = torch.randn((2, 10, 8))\n","mask = torch.randn((2, 10)) > 0.5\n","mask = mask.unsqueeze(1).unsqueeze(-1)\n","num_heads = 4\n","model = TransformerEncoderCell(8, num_heads, 32, 0.1)\n","y = model(x, mask)\n","assert len(x.shape) == len(y.shape)\n","for dim_x, dim_y in zip(x.shape, y.shape):\n","    assert dim_x == dim_y\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"fAPO0P3oHWZo"},"source":["### 2.4.2 Building an Encoder Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOc28jQpL9kd"},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    \"\"\"\n","    A full encoder consisting of a set of TransformerEncoderCell.\n","    \"\"\"\n","    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout: float=0.1):\n","        \"\"\"\n","        Inputs:\n","        - input_dim: Input dimension for each token in a sequence\n","        - num_heads: Number of attention heads in a multi-head attention module\n","        - ff_dim: The hidden dimension for a feedforward network\n","        - num_cells: Number of TransformerEncoderCells\n","        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n","          modules.\n","        \"\"\"\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.norm = None # LayerNorm layer\n","        self.cells = None # TransformerEncoderCells\n","\n","        ###########################################################################\n","        # TODO: Construct a nn.ModuleList to store a stack of                     #\n","        # TranformerEncoderCells. Check the documentation here of how to use it   #\n","        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n","\n","        # At the same time, define a layer normalization layer to process the     #\n","        # output of the entire encoder.                                           #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):\n","        \"\"\"\n","        Inputs:\n","        - x: Tensor of the shape BxLxC, where B is the batch size, L is the sequence length,\n","          and C is the channel dimension\n","        - mask: Tensor for multi-head attention\n","\n","        Return:\n","        - y: Tensor of the shape of BxLxC, which is the normalized output of the encoder\n","        \"\"\"\n","\n","        y = None\n","        ###########################################################################\n","        # TODO: Feed x into the stack of TransformerEncoderCells and then         #\n","        # normalize the output with layer norm.                                   #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return y\n"]},{"cell_type":"markdown","metadata":{"id":"lRToOfz-U7NX"},"source":["## 2.5 Transformer Classifier"]},{"cell_type":"markdown","metadata":{"id":"75PSJtbSbSlV"},"source":["Now, lets put this all the above describled modules together to make out classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jNGH9CHaU9Pv"},"outputs":[],"source":["class TransformerClassifier(nn.Module):\n","    \"\"\"\n","    A Transformer-based text classifier.\n","    \"\"\"\n","    def __init__(self,\n","            vocab_size: int, embed_dim: int, num_heads: int, trx_ff_dim: int,\n","            num_trx_cells: int, num_class: int, dropout: float=0.1, pad_token: int=0\n","        ):\n","        \"\"\"\n","        Inputs:\n","        - vocab_size: Vocabulary size, indicating how many tokens we have in total.\n","        - embed_dim: The dimension of word embeddings\n","        - num_heads: Number of attention heads in a multi-head attention module\n","        - trx_ff_dim: The hidden dimension for a feedforward network\n","        - num_trx_cells: Number of TransformerEncoderCells\n","        - dropout: Dropout ratio\n","        - pad_token: The index of the padding token.\n","        \"\"\"\n","        super(TransformerClassifier, self).__init__()\n","\n","        self.embed_dim = embed_dim\n","\n","        # word embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n","\n","        ###########################################################################\n","        # TODO: Define a module for positional encoding, Transformer encoder, and #\n","        # a output layer                                                          #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, text, mask=None):\n","        \"\"\"\n","        Inputs:\n","        - text: Tensor with the shape of BxLxC.\n","        - mask: Tensor for multi-head attention\n","\n","        Return:\n","        - logits: Tensor with the shape of BxK, where K is the number of classes\n","        \"\"\"\n","\n","        # word embeddings, note we multiple the embeddings by a factor\n","        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n","\n","        logits = None\n","        ###########################################################################\n","        # TODO: Apply positional embedding to the input, which is then fed into   #\n","        # the encoder. Average pooling is applied then to all the features of all #\n","        # tokens. Finally, the logits are computed based on the pooled features.  #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"810eXvOtU_KA"},"outputs":[],"source":["# Sanity Check\n","vocab_size = 10\n","embed_dim = 16\n","num_heads = 4\n","trx_ff_dim = 16\n","num_trx_cells = 2\n","num_class = 3\n","\n","x = torch.arange(vocab_size).view(1, -1)\n","x = torch.cat((x, x), dim=0)\n","mask = (x != 0).unsqueeze(-2).unsqueeze(1)\n","model = TransformerClassifier(vocab_size, embed_dim, num_heads, trx_ff_dim, num_trx_cells, num_class)\n","print('x: {}, mask: {}'.format(x.shape, mask.shape))\n","y = model(x, mask)\n","assert len(y.shape) == 2 and y.shape[0] == x.shape[0] and y.shape[1] == num_class\n","print(y.shape)"]},{"cell_type":"markdown","metadata":{"id":"Y6S703BMVr4S"},"source":["## 2.6 Deciding HyperParameters & Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oT2edMKVbHK"},"outputs":[],"source":["assert torch.cuda.is_available()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hyperparameters\n","epochs = 3 # epoch\n","lr = 0.0005  # learning rate\n","batch_size = 64 # batch size for training\n","\n","train_iter = AG_NEWS(split='train')\n","num_class = len(set([label for (label, text) in train_iter]))\n","vocab_size = len(vocab)\n","emsize = 64\n","\n","num_heads = 4\n","num_trx_cells = 2\n","\n","gradient_norm_clip = 1\n","\n","###########################################################################\n","# Define a Transformer-based text classifier and a loss function.         #\n","###########################################################################\n","raise NotImplementedError\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################\n","model = model.to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n","total_accu = None\n","\n","# You should be able to get a validation accuracy around 89%\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train(model, train_dataloader, loss_func, device, gradient_norm_clip, optimizer)\n","    accu_val = evaluate(model, valid_dataloader, loss_func, device)\n","    if total_accu is not None and total_accu > accu_val:\n","        scheduler.step()\n","    else:\n","        total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59)"]},{"cell_type":"markdown","metadata":{"id":"0UC00oQqdR8_"},"source":["# Section 3: Transformers - 'Attention is All you need' : Machine Translation"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"images\\Decoder-module.png\" width=500>"]},{"cell_type":"markdown","metadata":{"id":"lM6xGB79utJL"},"source":["The decoder module in a Seq-Seq Transformer model is responsible for generating the output sequence based on the information gathered by the encoder and previous tokens in an autoregressive manner. It utilizes self-attention mechanisms with masking to enforce causality and multi-head attention to capture dependencies between tokens in the output. Self-attention calculates attention weights for each position in the sequence, and multi-head attention aggregates the results from multiple attention heads, enhancing the model's representational power. Cross-attention is also employed to allow the decoder to focus on relevant parts of the encoder's output.\n","\n","Additionally, position-wise feed-forward networks further process the information by applying linear transformations and non-linear activation functions to each position independently. Throughout the decoder, layer normalization and residual connections are utilized to enhance training stability. These components are typically stacked in multiple layers to enable the model to learn complex relationships and generate coherent output sequences.\n"]},{"cell_type":"markdown","metadata":{"id":"-t22d9w_d4ia"},"source":["### 3.1.1 Decoder Cell"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQyVu87RdZfa"},"outputs":[],"source":["class TransformerDecoderCell(nn.Module):\n","    \"\"\"\n","    A single cell (unit) of the Transformer decoder.\n","    \"\"\"\n","    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, dropout: float=0.1):\n","        \"\"\"\n","        Inputs:\n","        - input_dim: Input dimension for each token in a sequence\n","        - num_heads: Number of attention heads in a multi-head attention module\n","        - ff_dim: The hidden dimension for a feedforward network\n","        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n","          modules.\n","        \"\"\"\n","        super(TransformerDecoderCell, self).__init__()\n","\n","        ###########################################################################\n","        # TODO: Similar to the TransformerEncoderCell, define two                 #\n","        # MultiHeadAttention modules. One for processing the tokens on the        #\n","        # decoder side. The other for getting the attention across the encoder.   #\n","        # and the decoder. Also define a feedforward network. Don't forget the    #\n","        # Dropout and Layer Norm layers.                                          #\n","        ###########################################################################\n","\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):\n","        \"\"\"\n","        Inputs:\n","        - x: Tensor of BxLdxC, word embeddings on the decoder side\n","        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n","        - src_mask: Tensor, masks of the tokens on the encoder side\n","        - tgt_mask: Tensor, masks of the tokens on the decoder side\n","\n","        Return:\n","        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n","        \"\"\"\n","\n","        y = None\n","        ###########################################################################\n","        # TODO: Compute the self-attended features for the tokens on the decoder  #\n","        # side. Then compute the corss-attended features for the tokens on the    #\n","        # decoder side to the encoded features, which are finally feed into the   #\n","        # feedforward network                                                     #\n","        ###########################################################################\n","\n","        raise NotImplementedError\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mwtQ02ReC7e"},"outputs":[],"source":["# Sanity Check\n","\n","dec_feats = torch.randn((3, 10, 16))\n","dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n","\n","enc_feats = torch.randn((3, 12, 16))\n","enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n","\n","model = TransformerDecoderCell(16, 2, 32, 0.1)\n","z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n","assert len(z.shape) == len(dec_feats.shape)\n","for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n","    assert dim_z == dim_x\n","print(z.shape)"]},{"cell_type":"markdown","metadata":{"id":"a5YzXlgzd6v_"},"source":["### 3.1.2 Building the Decoder Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xgx1x5_eeAGe"},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    \"\"\"\n","    A TransformerDecoder is a stack of multiple TransformerDecoderCells and a Layer Norm.\n","    \"\"\"\n","    def __init__(self, input_dim: int, num_heads: int, ff_dim: int, num_cells: int, dropout=0.1):\n","        \"\"\"\n","        Inputs:\n","        - input_dim: Input dimension for each token in a sequence\n","        - num_heads: Number of attention heads in a multi-head attention module\n","        - ff_dim: The hidden dimension for a feedforward network\n","        - num_cells: How many TransformerDecoderCells in stack\n","        - dropout: Dropout ratio for the output of the multi-head attention and feedforward\n","          modules.\n","        \"\"\"\n","        super(TransformerDecoder, self).__init__()\n","\n","        ###########################################################################\n","        # TODO: Construct a nn.ModuleList to store a stack of                     #\n","        # TranformerDecoderCells. Check the documentation here of how to use it   #\n","        # https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n","\n","        # At the same time, define a layer normalization layer to process the     #\n","        # output of the entire encoder.                                           #\n","        ###########################################################################\n","        raise NotImplementedError\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, src_mask=None, tgt_mask=None):\n","        \"\"\"\n","        Inputs:\n","        - x: Tensor of BxLdxC, word embeddings on the decoder side\n","        - encoder_output: Tensor of BxLexC, word embeddings on the encoder side\n","        - src_mask: Tensor, masks of the tokens on the encoder side\n","        - tgt_mask: Tensor, masks of the tokens on the decoder side\n","\n","        Return:\n","        - y: Tensor of BxLdxC. Attended features for all tokens on the decoder side.\n","        \"\"\"\n","\n","        y = None\n","        ###########################################################################\n","        # TODO: Feed x into the stack of TransformerDecoderCells and then         #\n","        # normalize the output with layer norm.                                   #\n","        ###########################################################################\n","        raise NotImplementedError\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"reAYOrJfeHEj"},"outputs":[],"source":["# Sanity Check\n","dec_feats = torch.randn((3, 10, 16))\n","dec_mask = torch.randn((3, 1, 10, 10)) > 0.5\n","\n","enc_feats = torch.randn((3, 12, 16))\n","enc_mask = torch.randn((3, 1, 1, 12)) > 0.5\n","\n","model = TransformerDecoder(16, 2, 32, 2, 0.1)\n","z = model(dec_feats, enc_feats, enc_mask, dec_mask)\n","assert len(z.shape) == len(dec_feats.shape)\n","for dim_z, dim_x in zip(z.shape, dec_feats.shape):\n","    assert dim_z == dim_x\n","print(z.shape)"]},{"cell_type":"markdown","metadata":{"id":"SW7rCvoBeIPx"},"source":["## 3.2 Transformer Based Seq-to-Seq Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLiV0CqNeOFm"},"outputs":[],"source":["class Seq2SeqTransformer(nn.Module):\n","    \"\"\"\n","    Transformer-based sequence-to-sequence model.\n","    \"\"\"\n","    def __init__(self,\n","            num_encoder_layers: int, num_decoder_layers: int, embed_dim: int,\n","            num_heads: int, src_vocab_size: int, tgt_vocab_size: int,\n","            trx_ff_dim: int = 512, dropout: float = 0.1, pad_token: int=0\n","        ):\n","        \"\"\"\n","        Inputs:\n","        - num_encoder_layers: How many TransformerEncoderCell in stack\n","        - num_decoder_layers: How many TransformerDecoderCell in stack\n","        - embed_dim: Word embeddings dimension\n","        - num_heads: Number of attention heads\n","        - src_vocab_size: Number of tokens in the source language vocabulary\n","        - tgt_vocab_size: Number of tokens in the target language vocabulary\n","        - trx_ff_dim: Hidden dimension in the feedforward network\n","        - dropout: Dropout ratio\n","        \"\"\"\n","        super(Seq2SeqTransformer, self).__init__()\n","\n","        self.embed_dim = embed_dim\n","\n","        # Word embeddings for both the source and target languages\n","        self.src_token_embed = nn.Embedding(src_vocab_size, embed_dim, padding_idx=pad_token)\n","        self.tgt_token_embed = nn.Embedding(tgt_vocab_size, embed_dim, padding_idx=pad_token)\n","\n","        ###########################################################################\n","        # TODO: Define the positional encoding, encoder, decoder, and the output  #\n","        # layer. Think of how many classes are in the output layer.               #\n","        ###########################################################################\n","        raise NotImplementedError\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor):\n","        \"\"\"\n","        Inputs:\n","        - src: Tensor of BxLe, word indexes in the source language\n","        - tgt: Tensor of BxLd, word indexes in the target language\n","        - src_mask: Tensor, masks of the tokens on the encoder side\n","        - tgt_mask: Tensor, masks of the tokens on the decoder side\n","\n","        Return:\n","        - y: Tensor of BxLdxK. K is the number of classes in the output.\n","        \"\"\"\n","\n","        # Get word embeddings. Note they are scaled.\n","        src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n","        tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n","\n","        logits = None\n","        ###########################################################################\n","        # TODO: Add positional encodings to the word embeddings. Feed them then   #\n","        # to the encoder and decoder, respectively. Get the logits finally.       #\n","        ###########################################################################\n","        raise NotImplementedError\n","\n","\n","\n","        ###########################################################################\n","        #                             END OF YOUR CODE                            #\n","        ###########################################################################\n","\n","        return logits\n","\n","    def encode(self, src:  torch.Tensor, src_mask:  torch.Tensor):\n","      src_embed = self.src_token_embed(src) * math.sqrt(self.embed_dim)\n","      return self.transformer_encoder(self.positional_encoding(src_embed), src_mask)\n","\n","    def decode(self, tgt:  torch.Tensor, memory:  torch.Tensor, src_mask:  torch.Tensor, tgt_mask:  torch.Tensor):\n","      tgt_embed = self.tgt_token_embed(tgt) * math.sqrt(self.embed_dim)\n","      return self.transformer_decoder(self.positional_encoding(tgt_embed), memory, src_mask, tgt_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V7Sr6AIeRzL"},"outputs":[],"source":["src_vocab_size = 10\n","src = torch.arange(src_vocab_size).view(1, -1)\n","src = torch.cat((src, src), dim=0)\n","src_mask = torch.randn((2, 1, 1, src_vocab_size)) > 0.5\n","\n","tgt_vocab_size = 12\n","tgt = torch.arange(tgt_vocab_size).view(1, -1)\n","tgt = torch.cat((tgt, tgt), dim=0)\n","tgt_mask = torch.randn((2, 1, tgt_vocab_size, tgt_vocab_size)) > 0.5\n","\n","model = Seq2SeqTransformer(2, 2, 16, 2, src_vocab_size, tgt_vocab_size, 32, 0.1, 0)\n","z = model(src, tgt, src_mask, tgt_mask)\n","print(z.shape)"]},{"cell_type":"markdown","metadata":{"id":"BqgiM4hden4B"},"source":["## Utility"]},{"cell_type":"markdown","metadata":{"id":"C2IKSRZ1eUDw"},"source":["Attention Mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUx8QDwieWZW"},"outputs":[],"source":["def subsequent_mask(size):\n","    \"Mask out subsequent positions.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0\n","\n","\n","def create_mask(src, tgt, pad_token=0):\n","    src_mask = (src != pad_token).unsqueeze(-2).unsqueeze(1)\n","\n","    tgt_seq_len = tgt.shape[0]\n","    tgt_mask = (tgt != pad_token).unsqueeze(-2)\n","    tgt_mask = tgt_mask & subsequent_mask(tgt.shape[1]).type_as(tgt_mask.data)\n","\n","    return src_mask, tgt_mask.unsqueeze(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1699287507771,"user":{"displayName":"Aanand .D","userId":"04055414698771147992"},"user_tz":300},"id":"iLielDIoeYAc","outputId":"39dae3bd-832c-420e-c762-8cad8a4b05ae"},"outputs":[],"source":["# Let's visualize what the target mask looks like\n","\n","sns.set_context(context=\"talk\")\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(subsequent_mask(20)[0].numpy())\n","\n","x = torch.arange(src_vocab_size).view(1, -1)\n","x = torch.cat((x, x), dim=0)\n","src_mask, tgt_mask = create_mask(x, x)\n","print(src_mask.shape, tgt_mask.shape)"]},{"cell_type":"markdown","metadata":{"id":"XxTO4EJted8m"},"source":["## Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcTVDIentHiu"},"outputs":[],"source":["from torchtext.datasets import multi30k, Multi30k\n","\n","# Update URLs to point to data stored by user\n","multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n","multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n","multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n","\n","# Update hash since there is a discrepancy between user hosted test split and that of the test split in the original dataset\n","multi30k.MD5[\"test\"] = \"6d1ca1dba99e2c5dd54cae1226ff11c2551e6ce63527ebb072a1f70f72a5cd36\"\n","\n","data_train = Multi30k(split='train')\n","data_val = Multi30k(split='valid')\n","data_test = Multi30k(split='test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HO56fT5JeeuX"},"outputs":[],"source":["SRC_LANGUAGE = 'de'\n","TGT_LANGUAGE = 'en'\n","\n","# Place-holders\n","token_transform = {}\n","vocab_transform = {}\n","\n","\n","# # Create source and target language tokenizer. Make sure to install the dependencies.\n","\n","token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","\n","# helper function to yield list of tokens\n","def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n","\n","    for data_sample in data_iter:\n","        yield token_transform[language](data_sample[language_index[language]])\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Training data Iterator\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    # Create torchtext's Vocab object\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","# Set UNK_IDX as the default index. This index is returned when the token is not found.\n","# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    vocab_transform[ln].set_default_index(UNK_IDX)\n","\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int]):\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","# src and tgt language text transforms to convert raw strings into tensors indices\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(\n","        token_transform[ln], #Tokenization\n","        vocab_transform[ln], #Numericalization\n","        tensor_transform # Add BOS/EOS and create tensor\n","    )\n","\n","\n","# function to collate data samples into batch tesors\n","def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch.transpose(0, 1), tgt_batch.transpose(0, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwZZas2Hk9zx"},"outputs":[],"source":["BATCH_SIZE = 8\n","\n","train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","for idx, (src, tgt) in enumerate(train_dataloader):\n","    if idx > 2:\n","        break\n","    print('src: {}, tgt: {}'.format(src.shape, tgt.shape))"]},{"cell_type":"markdown","metadata":{"id":"l3vT-8QKermB"},"source":["## 3.3 Model HyperParameters & Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fskKTE7Nevot"},"outputs":[],"source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMBED_SIZE = 512\n","NUM_ATTN_HEADS = 8\n","FF_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","transformer = None\n","###########################################################################\n","# TODO: Define the model and loss function.                               #\n","# Note that this time we will generate tokens, where some of them in the  #\n","# training time are from paddings. We don't want to penalize the model    #\n","# if the output at such positions are wrong. You can use the              #\n","# `ignore_index` in a loss function to suppress loss computation if the   #\n","# ground-truth label is equal to the given value. Check here for          #\n","# more details https://pytorch.org/docs/stable/nn.html#loss-functions     #\n","###########################################################################\n","raise NotImplementedError\n","\n","###########################################################################\n","#                             END OF YOUR CODE                            #\n","###########################################################################\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","transformer = transformer.to(device)\n","\n","optimizer = torch.optim.Adam(\n","    transformer.parameters(),\n","    lr=0.0001,\n","    betas=(0.9, 0.98),\n","    eps=1e-9\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tA7ZPoke4bV"},"outputs":[],"source":["def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        tgt_input = tgt[:, :-1]\n","\n","        src_mask, tgt_mask = create_mask(src, tgt_input)\n","        src_mask = src_mask.to(device)\n","        tgt_mask = tgt_mask.to(device)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[:, 1:]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(list(train_dataloader))\n","\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        tgt_input = tgt[:, :-1]\n","\n","        src_mask, tgt_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask)\n","\n","        tgt_out = tgt[:, 1:]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(list(val_dataloader))\n","\n","from timeit import default_timer as timer\n","\n","NUM_EPOCHS = 10\n","\n","# You should be able to get train loss around 1.5 and val loss around 2.2\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"]},{"cell_type":"markdown","metadata":{"id":"r29zkV8YA3iC"},"source":["## Greedy Translate/Decode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsiMgRWwjNbQ"},"outputs":[],"source":["def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    src = src.to(device)\n","    src_mask = src_mask.to(device)\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.float).to(device)\n","    for i in range(max_len-1):\n","\n","        memory = memory.to(device)\n","        tgt_mask = (subsequent_mask(ys.size(1))\n","                    .type(torch.bool)).to(device)\n","        cross_mask = torch.ones(1,i + 1,src.size(1), device=device)\n","        out = model.decode(ys.long(), memory, cross_mask, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.output_layer(out[:, -1])\n","        _, next_word = torch.max(prob[-1,:], dim=0)\n","        next_word = next_word.item()\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","# actual function to translate input sentence into target language\n","def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n","    num_tokens = src.shape[1]\n","    src_mask = (torch.zeros(1, num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens, model = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX)\n","\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.flatten().cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6Qii2NvBvQ4"},"outputs":[],"source":["src_sentence = \"Eine Gruppe von Menschen steht vor einem Iglu .\"\n","translate(transformer, src_sentence)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
